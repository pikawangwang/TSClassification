{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "25b00577-065d-4f6b-bfde-d19214ab4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pypots.classification import GRUD\n",
    "from pypots.optim import Adam\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3b3fb5aa-a768-4635-99fa-7e6198aeb2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: {'X': array([[[0.25961536, 0.28846157, 0.2990654 , ..., 0.45512822,\n",
      "         0.6052632 , 0.57723576],\n",
      "        [0.27884614, 0.30769232, 0.32710278, ..., 0.45512822,\n",
      "         0.56578946, 0.56097555],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        ...,\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.4134615 , 0.53846157, 0.3084112 , ..., 0.64102566,\n",
      "         0.65789473, 0.699187  ],\n",
      "        [0.4134615 , 0.53846157, 0.2990654 , ...,        nan,\n",
      "                nan, 0.7479674 ],\n",
      "        [       nan, 0.53846157, 0.33644858, ..., 0.71153843,\n",
      "         0.6973684 , 0.7479674 ],\n",
      "        ...,\n",
      "        [0.38461536, 0.5       , 0.27102804, ..., 0.65384614,\n",
      "         0.68421054, 0.699187  ],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.5673077 , 0.8076923 , 0.5700934 , ..., 0.41666666,\n",
      "         0.31578946, 0.47154468],\n",
      "        [0.54807687, 0.7692308 ,        nan, ..., 0.3653846 ,\n",
      "         0.28947365, 0.4390244 ],\n",
      "        [0.54807687, 0.78846157, 0.5607476 , ..., 0.40384617,\n",
      "         0.31578946, 0.4634146 ],\n",
      "        ...,\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [0.50961536, 0.7307692 , 0.5233644 , ..., 0.3653846 ,\n",
      "                nan,        nan],\n",
      "        [0.50961536, 0.7307692 , 0.5233644 , ..., 0.3269231 ,\n",
      "                nan, 0.40650403]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.14423077, 0.21153846, 0.1588785 , ..., 0.21794873,\n",
      "                nan, 0.3333333 ],\n",
      "        [0.14423077, 0.21153846, 0.1588785 , ..., 0.20512822,\n",
      "         0.25      ,        nan],\n",
      "        [0.15384613, 0.23076922, 0.1682243 , ..., 0.23717949,\n",
      "         0.2763158 , 0.34959346],\n",
      "        ...,\n",
      "        [0.16346152, 0.23076922, 0.17757007, ..., 0.25      ,\n",
      "         0.28947365, 0.35772356],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.20192306, 0.28846157, 0.18691587, ...,        nan,\n",
      "         0.31578946, 0.3902439 ],\n",
      "        [0.1826923 , 0.26923075, 0.1682243 , ..., 0.25641027,\n",
      "         0.28947365, 0.36585364],\n",
      "        [       nan, 0.26923075, 0.17757007, ..., 0.26923075,\n",
      "         0.30263156, 0.3739837 ],\n",
      "        ...,\n",
      "        [0.20192306,        nan, 0.18691587, ..., 0.30769232,\n",
      "         0.32894737, 0.40650403],\n",
      "        [0.20192306, 0.28846157, 0.18691587, ..., 0.33333334,\n",
      "         0.35526317, 0.43089426],\n",
      "        [0.20192306, 0.28846157, 0.19626167, ..., 0.3269231 ,\n",
      "         0.34210527, 0.4146341 ]],\n",
      "\n",
      "       [[0.42307693, 0.57692313, 0.26168224, ..., 0.48076925,\n",
      "         0.5394737 , 0.57723576],\n",
      "        [0.4134615 , 0.5576923 , 0.22429904, ..., 0.46153846,\n",
      "         0.5131579 , 0.5447154 ],\n",
      "        [0.39423078, 0.53846157, 0.21495327, ...,        nan,\n",
      "         0.4868421 , 0.5121951 ],\n",
      "        ...,\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]]], dtype=float32), 'y': array([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0]), 'mask': array([[[False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False,  True, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ...,  True, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False]],\n",
      "\n",
      "       [[False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ...,  True,  True, False],\n",
      "        [ True, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ...,  True, False, False]],\n",
      "\n",
      "       [[False, False, False, ..., False, False, False],\n",
      "        [False, False,  True, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False,  True,  True],\n",
      "        [False, False, False, ..., False,  True, False]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[False, False, False, ..., False,  True, False],\n",
      "        [False, False, False, ..., False, False,  True],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False]],\n",
      "\n",
      "       [[False, False, False, ...,  True, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [ True, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False,  True, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False]],\n",
      "\n",
      "       [[False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ...,  True, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False,  True, ..., False, False, False]]])}\n",
      "Validation dataset: {'X': array([[[0.17307691,        nan, 0.24299064, ...,        nan,\n",
      "         0.34210527, 0.39837396],\n",
      "        [0.17307691, 0.19230768,        nan, ..., 0.35897437,\n",
      "         0.36842108, 0.42276418],\n",
      "        [0.17307691, 0.19230768, 0.26168224, ..., 0.39102563,\n",
      "         0.39473683, 0.4390244 ],\n",
      "        ...,\n",
      "        [0.21153845, 0.23076922,        nan, ..., 0.65384614,\n",
      "         0.59210527, 0.6504065 ],\n",
      "        [0.1826923 , 0.19230768, 0.2897196 , ..., 0.5576923 ,\n",
      "         0.5131579 , 0.5691057 ],\n",
      "        [0.20192306, 0.21153846, 0.2803738 , ..., 0.5448718 ,\n",
      "         0.5263158 , 0.58536583]],\n",
      "\n",
      "       [[0.17307691, 0.25      , 0.19626167, ...,        nan,\n",
      "         0.30263156,        nan],\n",
      "        [0.15384613, 0.23076922, 0.17757007, ..., 0.2628205 ,\n",
      "         0.28947365, 0.36585364],\n",
      "        [0.16346152, 0.23076922, 0.17757007, ..., 0.27564105,\n",
      "                nan, 0.3739837 ],\n",
      "        ...,\n",
      "        [0.14423077, 0.21153846, 0.1588785 , ..., 0.23717949,\n",
      "         0.2763158 , 0.3414634 ],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.20192306, 0.23076922, 0.20560747, ..., 0.3269231 ,\n",
      "         0.40789473, 0.4146341 ],\n",
      "        [0.1923077 , 0.21153846, 0.19626167, ..., 0.35897437,\n",
      "                nan, 0.44715446],\n",
      "        [       nan, 0.17307693,        nan, ..., 0.3269231 ,\n",
      "         0.44736844, 0.44715446],\n",
      "        ...,\n",
      "        [0.24038462, 0.26923075, 0.27102804, ..., 0.23076925,\n",
      "         0.2763158 , 0.34959346],\n",
      "        [       nan, 0.25      , 0.25233644, ..., 0.23717949,\n",
      "         0.2763158 , 0.34959346],\n",
      "        [0.1923077 , 0.21153846, 0.21495327, ..., 0.24358976,\n",
      "         0.2631579 , 0.34959346]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[       nan, 0.78846157,        nan, ..., 0.57692313,\n",
      "         0.6052632 , 0.63414633],\n",
      "        [       nan, 0.6538462 , 0.6728972 , ..., 0.39743593,\n",
      "         0.36842108, 0.44715446],\n",
      "        [0.50961536, 0.53846157, 0.635514  , ..., 0.5192307 ,\n",
      "         0.4868421 , 0.5121951 ],\n",
      "        ...,\n",
      "        [0.75961536, 0.8269231 , 0.6074766 , ..., 0.5897436 ,\n",
      "         0.59210527, 0.63414633],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.22115384, 0.28846157, 0.2990654 , ..., 0.4871795 ,\n",
      "         0.4736842 , 0.53658533],\n",
      "        [0.26923072, 0.36538464, 0.36448595, ..., 0.5256411 ,\n",
      "         0.4605263 , 0.5447154 ],\n",
      "        [0.24999999, 0.32692307, 0.32710278, ..., 0.5192307 ,\n",
      "         0.4736842 , 0.5528455 ],\n",
      "        ...,\n",
      "        [0.27884614,        nan, 0.36448595, ..., 0.55128205,\n",
      "         0.4736842 , 0.5691057 ],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.3076923 , 0.34615386, 0.33644858, ...,        nan,\n",
      "         0.5       ,        nan],\n",
      "        [0.27884614, 0.30769232,        nan, ..., 0.4679487 ,\n",
      "         0.42105263, 0.53658533],\n",
      "        [0.26923072, 0.30769232, 0.3084112 , ..., 0.6025641 ,\n",
      "         0.57894737, 0.6504065 ],\n",
      "        ...,\n",
      "        [0.33653843, 0.38461542, 0.39252338, ..., 0.5256411 ,\n",
      "         0.39473683, 0.5203252 ],\n",
      "        [0.29807693, 0.34615386, 0.34579438, ..., 0.4871795 ,\n",
      "         0.40789473, 0.50406504],\n",
      "        [0.33653843, 0.38461542, 0.40186915, ..., 0.57692313,\n",
      "         0.43421054, 0.56097555]]], dtype=float32), 'y': array([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]), 'mask': array([[[False,  True, False, ...,  True, False, False],\n",
      "        [False, False,  True, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False,  True, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False]],\n",
      "\n",
      "       [[False, False, False, ...,  True, False,  True],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False,  True, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False,  True, False],\n",
      "        [False,  True, False, ..., False, False, False]],\n",
      "\n",
      "       [[False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False,  True, False],\n",
      "        [ True, False,  True, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [ True, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ True, False,  True, ..., False, False, False],\n",
      "        [ True, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False,  True, False],\n",
      "        [ True, False, False, ..., False, False, False]],\n",
      "\n",
      "       [[False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False,  True, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False]],\n",
      "\n",
      "       [[False, False, False, ...,  True, False,  True],\n",
      "        [False, False,  True, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False]]]), 'X_ori': array([[[0.17307691, 0.19230768, 0.24299064, ..., 0.3269231 ,\n",
      "         0.34210527, 0.39837396],\n",
      "        [0.17307691, 0.19230768, 0.26168224, ..., 0.35897437,\n",
      "         0.36842108, 0.42276418],\n",
      "        [0.17307691, 0.19230768, 0.26168224, ..., 0.39102563,\n",
      "         0.39473683, 0.4390244 ],\n",
      "        ...,\n",
      "        [0.21153845, 0.23076922, 0.2990654 , ..., 0.65384614,\n",
      "         0.59210527, 0.6504065 ],\n",
      "        [0.1826923 , 0.19230768, 0.2897196 , ..., 0.5576923 ,\n",
      "         0.5131579 , 0.5691057 ],\n",
      "        [0.20192306, 0.21153846, 0.2803738 , ..., 0.5448718 ,\n",
      "         0.5263158 , 0.58536583]],\n",
      "\n",
      "       [[0.17307691, 0.25      , 0.19626167, ..., 0.30128208,\n",
      "         0.30263156, 0.3821138 ],\n",
      "        [0.15384613, 0.23076922, 0.17757007, ..., 0.2628205 ,\n",
      "         0.28947365, 0.36585364],\n",
      "        [0.16346152, 0.23076922, 0.17757007, ..., 0.27564105,\n",
      "         0.30263156, 0.3739837 ],\n",
      "        ...,\n",
      "        [0.14423077, 0.21153846, 0.1588785 , ..., 0.23717949,\n",
      "         0.2763158 , 0.3414634 ],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.20192306, 0.23076922, 0.20560747, ..., 0.3269231 ,\n",
      "         0.40789473, 0.4146341 ],\n",
      "        [0.1923077 , 0.21153846, 0.19626167, ..., 0.35897437,\n",
      "         0.44736844, 0.44715446],\n",
      "        [0.16346152, 0.17307693, 0.1682243 , ..., 0.3269231 ,\n",
      "         0.44736844, 0.44715446],\n",
      "        ...,\n",
      "        [0.24038462, 0.26923075, 0.27102804, ..., 0.23076925,\n",
      "         0.2763158 , 0.34959346],\n",
      "        [0.2307692 , 0.25      , 0.25233644, ..., 0.23717949,\n",
      "         0.2763158 , 0.34959346],\n",
      "        [0.1923077 , 0.21153846, 0.21495327, ..., 0.24358976,\n",
      "         0.2631579 , 0.34959346]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.7211538 , 0.78846157, 0.5794393 , ..., 0.57692313,\n",
      "         0.6052632 , 0.63414633],\n",
      "        [0.5961538 , 0.6538462 , 0.6728972 , ..., 0.39743593,\n",
      "         0.36842108, 0.44715446],\n",
      "        [0.50961536, 0.53846157, 0.635514  , ..., 0.5192307 ,\n",
      "         0.4868421 , 0.5121951 ],\n",
      "        ...,\n",
      "        [0.75961536, 0.8269231 , 0.6074766 , ..., 0.5897436 ,\n",
      "         0.59210527, 0.63414633],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.22115384, 0.28846157, 0.2990654 , ..., 0.4871795 ,\n",
      "         0.4736842 , 0.53658533],\n",
      "        [0.26923072, 0.36538464, 0.36448595, ..., 0.5256411 ,\n",
      "         0.4605263 , 0.5447154 ],\n",
      "        [0.24999999, 0.32692307, 0.32710278, ..., 0.5192307 ,\n",
      "         0.4736842 , 0.5528455 ],\n",
      "        ...,\n",
      "        [0.27884614, 0.38461542, 0.36448595, ..., 0.55128205,\n",
      "         0.4736842 , 0.5691057 ],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.3076923 , 0.34615386, 0.33644858, ..., 0.56410253,\n",
      "         0.5       , 0.60975605],\n",
      "        [0.27884614, 0.30769232, 0.33644858, ..., 0.4679487 ,\n",
      "         0.42105263, 0.53658533],\n",
      "        [0.26923072, 0.30769232, 0.3084112 , ..., 0.6025641 ,\n",
      "         0.57894737, 0.6504065 ],\n",
      "        ...,\n",
      "        [0.33653843, 0.38461542, 0.39252338, ..., 0.5256411 ,\n",
      "         0.39473683, 0.5203252 ],\n",
      "        [0.29807693, 0.34615386, 0.34579438, ..., 0.4871795 ,\n",
      "         0.40789473, 0.50406504],\n",
      "        [0.33653843, 0.38461542, 0.40186915, ..., 0.57692313,\n",
      "         0.43421054, 0.56097555]]], dtype=float32)}\n",
      "Testing dataset: {'X': array([[[0.46153843, 0.46153843, 0.6074766 , ..., 0.45512822,\n",
      "         0.6052632 , 0.601626  ],\n",
      "        [0.45192307, 0.46153843, 0.61682236, ..., 0.44871798,\n",
      "         0.59210527, 0.5934959 ],\n",
      "        [0.42307693, 0.42307693, 0.5794393 , ...,        nan,\n",
      "         0.6315789 , 0.6178861 ],\n",
      "        ...,\n",
      "        [0.47115386, 0.48076928, 0.6074766 , ..., 0.41025642,\n",
      "         0.55263156, 0.56097555],\n",
      "        [0.45192307, 0.46153843, 0.5607476 , ..., 0.39102563,\n",
      "         0.5394737 , 0.53658533],\n",
      "        [0.45192307, 0.46153843, 0.5794393 , ..., 0.43589744,\n",
      "                nan, 0.58536583]],\n",
      "\n",
      "       [[0.3076923 , 0.40384614, 0.36448595, ..., 0.48076925,\n",
      "         0.4736842 , 0.53658533],\n",
      "        [0.32692307, 0.42307693, 0.39252338, ..., 0.5       ,\n",
      "                nan, 0.53658533],\n",
      "        [0.34615386, 0.4423077 , 0.41121492, ..., 0.50641024,\n",
      "         0.4868421 , 0.5447154 ],\n",
      "        ...,\n",
      "        [0.32692307, 0.42307693, 0.36448595, ...,        nan,\n",
      "         0.5131579 , 0.56097555],\n",
      "        [0.31730765, 0.40384614, 0.35514018, ..., 0.49358973,\n",
      "         0.5       , 0.5528455 ],\n",
      "        [0.31730765, 0.40384614, 0.35514018, ..., 0.474359  ,\n",
      "         0.4868421 , 0.53658533]],\n",
      "\n",
      "       [[0.24038462, 0.26923075, 0.37383178, ..., 0.6346154 ,\n",
      "         0.56578946, 0.62601626],\n",
      "        [0.20192306, 0.23076922, 0.32710278, ..., 0.6153846 ,\n",
      "         0.59210527, 0.62601626],\n",
      "        [0.24038462, 0.26923075, 0.36448595, ..., 0.69871795,\n",
      "         0.6315789 , 0.699187  ],\n",
      "        ...,\n",
      "        [0.24038462, 0.26923075, 0.39252338, ..., 0.6025641 ,\n",
      "         0.56578946,        nan],\n",
      "        [0.24038462,        nan, 0.39252338, ..., 0.6025641 ,\n",
      "         0.56578946, 0.6178861 ],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.24038462, 0.32692307, 0.33644858, ..., 0.49358973,\n",
      "         0.5       , 0.5447154 ],\n",
      "        [0.2307692 , 0.30769232, 0.33644858, ..., 0.50641024,\n",
      "         0.4605263 ,        nan],\n",
      "        [0.24038462, 0.30769232, 0.33644858, ..., 0.5256411 ,\n",
      "         0.4868421 , 0.5528455 ],\n",
      "        ...,\n",
      "        [0.24038462, 0.32692307, 0.34579438, ..., 0.50641024,\n",
      "                nan, 0.53658533],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.14423077, 0.21153846, 0.1308411 , ..., 0.25      ,\n",
      "         0.30263156, 0.3739837 ],\n",
      "        [       nan,        nan, 0.08411215, ..., 0.25641027,\n",
      "         0.36842108, 0.39837396],\n",
      "        [       nan, 0.15384617, 0.09345794, ..., 0.25641027,\n",
      "         0.35526317, 0.3902439 ],\n",
      "        ...,\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.45192307,        nan, 0.58878505, ..., 0.39102563,\n",
      "         0.5394737 , 0.53658533],\n",
      "        [0.44230765, 0.4423077 , 0.58878505, ..., 0.37820515,\n",
      "         0.5263158 , 0.53658533],\n",
      "        [0.44230765, 0.4423077 , 0.58878505, ..., 0.40384617,\n",
      "         0.55263156,        nan],\n",
      "        ...,\n",
      "        [0.44230765, 0.4423077 , 0.5794393 , ..., 0.42307696,\n",
      "         0.57894737,        nan],\n",
      "        [0.4326923 , 0.4423077 , 0.5794393 , ..., 0.41025642,\n",
      "         0.57894737, 0.5691057 ],\n",
      "        [0.4134615 , 0.40384614, 0.5607476 , ..., 0.40384617,\n",
      "         0.57894737, 0.5691057 ]]], dtype=float32), 'y': array([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0]), 'mask': array([[[False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ...,  True, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False,  True, False]],\n",
      "\n",
      "       [[False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False,  True, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ...,  True, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False]],\n",
      "\n",
      "       [[False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False,  True],\n",
      "        [False,  True, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False,  True],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False,  True, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [ True,  True, False, ..., False, False, False]],\n",
      "\n",
      "       [[False, False, False, ..., False, False, False],\n",
      "        [ True,  True, False, ..., False, False, False],\n",
      "        [ True, False, False, ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False,  True]],\n",
      "\n",
      "       [[False,  True, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False,  True],\n",
      "        ...,\n",
      "        [False, False, False, ..., False, False,  True],\n",
      "        [False, False, False, ..., False, False, False],\n",
      "        [False, False, False, ..., False, False, False]]]), 'X_ori': array([[[0.46153843, 0.46153843, 0.6074766 , ..., 0.45512822,\n",
      "         0.6052632 , 0.601626  ],\n",
      "        [0.45192307, 0.46153843, 0.61682236, ..., 0.44871798,\n",
      "         0.59210527, 0.5934959 ],\n",
      "        [0.42307693, 0.42307693, 0.5794393 , ..., 0.46153846,\n",
      "         0.6315789 , 0.6178861 ],\n",
      "        ...,\n",
      "        [0.47115386, 0.48076928, 0.6074766 , ..., 0.41025642,\n",
      "         0.55263156, 0.56097555],\n",
      "        [0.45192307, 0.46153843, 0.5607476 , ..., 0.39102563,\n",
      "         0.5394737 , 0.53658533],\n",
      "        [0.45192307, 0.46153843, 0.5794393 , ..., 0.43589744,\n",
      "         0.59210527, 0.58536583]],\n",
      "\n",
      "       [[0.3076923 , 0.40384614, 0.36448595, ..., 0.48076925,\n",
      "         0.4736842 , 0.53658533],\n",
      "        [0.32692307, 0.42307693, 0.39252338, ..., 0.5       ,\n",
      "         0.4605263 , 0.53658533],\n",
      "        [0.34615386, 0.4423077 , 0.41121492, ..., 0.50641024,\n",
      "         0.4868421 , 0.5447154 ],\n",
      "        ...,\n",
      "        [0.32692307, 0.42307693, 0.36448595, ..., 0.50641024,\n",
      "         0.5131579 , 0.56097555],\n",
      "        [0.31730765, 0.40384614, 0.35514018, ..., 0.49358973,\n",
      "         0.5       , 0.5528455 ],\n",
      "        [0.31730765, 0.40384614, 0.35514018, ..., 0.474359  ,\n",
      "         0.4868421 , 0.53658533]],\n",
      "\n",
      "       [[0.24038462, 0.26923075, 0.37383178, ..., 0.6346154 ,\n",
      "         0.56578946, 0.62601626],\n",
      "        [0.20192306, 0.23076922, 0.32710278, ..., 0.6153846 ,\n",
      "         0.59210527, 0.62601626],\n",
      "        [0.24038462, 0.26923075, 0.36448595, ..., 0.69871795,\n",
      "         0.6315789 , 0.699187  ],\n",
      "        ...,\n",
      "        [0.24038462, 0.26923075, 0.39252338, ..., 0.6025641 ,\n",
      "         0.56578946, 0.6178861 ],\n",
      "        [0.24038462, 0.26923075, 0.39252338, ..., 0.6025641 ,\n",
      "         0.56578946, 0.6178861 ],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.24038462, 0.32692307, 0.33644858, ..., 0.49358973,\n",
      "         0.5       , 0.5447154 ],\n",
      "        [0.2307692 , 0.30769232, 0.33644858, ..., 0.50641024,\n",
      "         0.4605263 , 0.53658533],\n",
      "        [0.24038462, 0.30769232, 0.33644858, ..., 0.5256411 ,\n",
      "         0.4868421 , 0.5528455 ],\n",
      "        ...,\n",
      "        [0.24038462, 0.32692307, 0.34579438, ..., 0.50641024,\n",
      "         0.4605263 , 0.53658533],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.14423077, 0.21153846, 0.1308411 , ..., 0.25      ,\n",
      "         0.30263156, 0.3739837 ],\n",
      "        [0.09615386, 0.13461539, 0.08411215, ..., 0.25641027,\n",
      "         0.36842108, 0.39837396],\n",
      "        [0.09615386, 0.15384617, 0.09345794, ..., 0.25641027,\n",
      "         0.35526317, 0.3902439 ],\n",
      "        ...,\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan],\n",
      "        [       nan,        nan,        nan, ...,        nan,\n",
      "                nan,        nan]],\n",
      "\n",
      "       [[0.45192307, 0.46153843, 0.58878505, ..., 0.39102563,\n",
      "         0.5394737 , 0.53658533],\n",
      "        [0.44230765, 0.4423077 , 0.58878505, ..., 0.37820515,\n",
      "         0.5263158 , 0.53658533],\n",
      "        [0.44230765, 0.4423077 , 0.58878505, ..., 0.40384617,\n",
      "         0.55263156, 0.5528455 ],\n",
      "        ...,\n",
      "        [0.44230765, 0.4423077 , 0.5794393 , ..., 0.42307696,\n",
      "         0.57894737, 0.57723576],\n",
      "        [0.4326923 , 0.4423077 , 0.5794393 , ..., 0.41025642,\n",
      "         0.57894737, 0.5691057 ],\n",
      "        [0.4134615 , 0.40384614, 0.5607476 , ..., 0.40384617,\n",
      "         0.57894737, 0.5691057 ]]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def introduce_missing_values(data, missing_rate):\n",
    "    \"\"\"在数据集中引入指定比例的缺失值并生成掩码矩阵\"\"\"\n",
    "    mask = np.random.rand(*data.shape) < missing_rate\n",
    "    data_with_missing = data.copy()\n",
    "    data_with_missing[mask] = np.nan\n",
    "    return data_with_missing, mask\n",
    "\n",
    "def gene_HypotensionData_custom(train_X, val_X, test_X, artificially_missing_rate: float = 0.1):\n",
    "    train_X_missing, train_mask = introduce_missing_values(train_X, artificially_missing_rate)\n",
    "    val_X_missing, val_mask = introduce_missing_values(val_X, artificially_missing_rate)\n",
    "    test_X_missing, test_mask = introduce_missing_values(test_X, artificially_missing_rate)\n",
    "    \n",
    "    dataset = {\n",
    "        'train_X': train_X_missing,\n",
    "        'val_X': val_X_missing,\n",
    "        'val_X_ori': val_X,\n",
    "        'test_X': test_X_missing,\n",
    "        'test_X_ori': test_X,\n",
    "        'train_mask': train_mask,\n",
    "        'val_mask': val_mask,\n",
    "        'test_mask': test_mask\n",
    "    }\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# 加载数据\n",
    "X_train = np.load('X_train_nan.npy', allow_pickle=True)\n",
    "Y_train = np.load('Y_train_nan.npy', allow_pickle=True)\n",
    "X_val = np.load('X_val_nan.npy', allow_pickle=True)\n",
    "Y_val = np.load('Y_val_nan.npy', allow_pickle=True)\n",
    "X_test = np.load('X_test_nan.npy', allow_pickle=True)\n",
    "Y_test = np.load('Y_test_nan.npy', allow_pickle=True)\n",
    "\n",
    "# 交换第二、三维\n",
    "X_train = np.transpose(X_train, (0, 2, 1))\n",
    "X_val = np.transpose(X_val, (0, 2, 1))\n",
    "X_test = np.transpose(X_test, (0, 2, 1))\n",
    "\n",
    "# 使用自定义函数处理数据集\n",
    "HypotensionData_dataset = gene_HypotensionData_custom(X_train, X_val, X_test, artificially_missing_rate=0.1)\n",
    "\n",
    "# 构建数据集格式\n",
    "dataset_for_training = {\n",
    "    \"X\": HypotensionData_dataset['train_X'],\n",
    "    \"y\": Y_train,\n",
    "    \"mask\": HypotensionData_dataset['train_mask']\n",
    "}\n",
    "\n",
    "dataset_for_validating = {\n",
    "    \"X\": HypotensionData_dataset['val_X'],\n",
    "    \"y\": Y_val,\n",
    "    \"mask\": HypotensionData_dataset['val_mask'],\n",
    "    \"X_ori\": HypotensionData_dataset['val_X_ori']\n",
    "}\n",
    "\n",
    "dataset_for_testing = {\n",
    "    \"X\": HypotensionData_dataset['test_X'],\n",
    "    \"y\": Y_test,\n",
    "    \"mask\": HypotensionData_dataset['test_mask'],\n",
    "    \"X_ori\": HypotensionData_dataset['test_X_ori']\n",
    "}\n",
    "\n",
    "# 打印检查\n",
    "print(\"Training dataset:\", dataset_for_training)\n",
    "print(\"Validation dataset:\", dataset_for_validating)\n",
    "print(\"Testing dataset:\", dataset_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e48f1af8-4861-4c2e-986c-81e403c4ad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 15:35:57 [INFO]: No given device, using default device: cpu\n",
      "2024-07-28 15:35:57 [INFO]: Model files will be saved to tutorial_results/imputation/timesnet\\20240728_T153557\n",
      "2024-07-28 15:35:57 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/timesnet\\20240728_T153557\\tensorboard\n",
      "2024-07-28 15:35:57 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,636,235\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.imputation import TimesNet\n",
    "\n",
    "# initialize the model\n",
    "timesnet = TimesNet(\n",
    "    n_steps=30,\n",
    "    n_features=11,\n",
    "    n_layers=1,\n",
    "    top_k=1,\n",
    "    d_model=128,\n",
    "    d_ffn=512,\n",
    "    n_kernels=5,\n",
    "    dropout=0.5,\n",
    "    apply_nonstationary_norm=False,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files \n",
    "    saving_path=\"tutorial_results/imputation/timesnet\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f60e7f22-9a06-4ed6-b205-431e31811ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 15:36:08 [INFO]: Epoch 001 - training loss: 0.2258, validation loss: 0.0309\n",
      "2024-07-28 15:36:08 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet_epoch1_loss0.03094053516785304.pypots\n",
      "2024-07-28 15:36:18 [INFO]: Epoch 002 - training loss: 0.0552, validation loss: 0.0176\n",
      "2024-07-28 15:36:18 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet_epoch2_loss0.01757576937476794.pypots\n",
      "2024-07-28 15:36:28 [INFO]: Epoch 003 - training loss: 0.0233, validation loss: 0.0156\n",
      "2024-07-28 15:36:28 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet_epoch3_loss0.015597327922781309.pypots\n",
      "2024-07-28 15:36:38 [INFO]: Epoch 004 - training loss: 0.0185, validation loss: 0.0136\n",
      "2024-07-28 15:36:38 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet_epoch4_loss0.01360697237153848.pypots\n",
      "2024-07-28 15:36:50 [INFO]: Epoch 005 - training loss: 0.0163, validation loss: 0.0126\n",
      "2024-07-28 15:36:50 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet_epoch5_loss0.012550671895345053.pypots\n",
      "2024-07-28 15:37:00 [INFO]: Epoch 006 - training loss: 0.0147, validation loss: 0.0107\n",
      "2024-07-28 15:37:00 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet_epoch6_loss0.010743019791940847.pypots\n",
      "2024-07-28 15:37:10 [INFO]: Epoch 007 - training loss: 0.0128, validation loss: 0.0097\n",
      "2024-07-28 15:37:10 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet_epoch7_loss0.00968201303233703.pypots\n",
      "2024-07-28 15:37:21 [INFO]: Epoch 008 - training loss: 0.0115, validation loss: 0.0094\n",
      "2024-07-28 15:37:21 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet_epoch8_loss0.009413142688572407.pypots\n",
      "2024-07-28 15:37:32 [INFO]: Epoch 009 - training loss: 0.0110, validation loss: 0.0085\n",
      "2024-07-28 15:37:32 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet_epoch9_loss0.00854932920386394.pypots\n",
      "2024-07-28 15:37:43 [INFO]: Epoch 010 - training loss: 0.0104, validation loss: 0.0082\n",
      "2024-07-28 15:37:43 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet_epoch10_loss0.00815680498878161.pypots\n",
      "2024-07-28 15:37:43 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2024-07-28 15:37:43 [INFO]: Saved the model to tutorial_results/imputation/timesnet\\20240728_T153557\\TimesNet.pypots\n"
     ]
    }
   ],
   "source": [
    "timesnet.fit(train_set=dataset_for_training, val_set=dataset_for_validating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7a8c4418-8527-4e6a-bd06-f4f93a2a5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesnet_results = timesnet.predict(dataset_for_testing)\n",
    "timesnet_imputation = timesnet_results[\"imputation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "71ff75b3-e501-4cd6-83eb-551876a8fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicating_mask = np.isnan(HypotensionData_dataset['test_X']) ^ np.isnan(HypotensionData_dataset['test_X_ori'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6c82e5e4-9d0d-4633-90ca-b43de4e3f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.0760\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import calc_mae\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = calc_mae(\n",
    "    timesnet_imputation,\n",
    "    np.nan_to_num(HypotensionData_dataset['test_X_ori']),\n",
    "    indicating_mask,\n",
    ")\n",
    "print(f\"Testing mean absolute error: {testing_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fdd583f0-6167-4367-9d92-df09ec22972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_train_X = timesnet.impute({\"X\": dataset_for_training[\"X\"]})\n",
    "imputed_val_X = timesnet.impute({\"X\": dataset_for_validating[\"X\"]})\n",
    "imputed_test_X = timesnet.impute({\"X\": dataset_for_testing[\"X\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d822081f-b491-4afb-b400-11a62e891503",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('imputed_X_train.npy', imputed_train_X)\n",
    "np.save('imputed_X_val.npy', imputed_val_X)\n",
    "np.save('imputed_X_test.npy', imputed_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df71427-c4a5-4577-9f8c-5724a2c11c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TSclassification",
   "language": "python",
   "name": "tsclassification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
